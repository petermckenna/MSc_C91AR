---
title: "Computer lab 7: The Relationship Between Correlation and Regression"
subtitle: "C91AR: Advanced Statistics using R"
title-slide-attributes:
    data-background-image: images/anime_programming.jpg
    data-background-size: 20%
    data-background-position: "50% 8%"
author: "Dr Pete McKenna"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
format: 
  revealjs:
    code-block-height: 300px
    slide-number: true
    logo: sossdoc-logo.png
    css: peter_style.css
    theme: [default, my_quarto_theme.scss]
editor: 
  markdown: 
    wrap: 72
bibliography: ref.bib 
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = T, 
                       cache = F,
                       eval = T,    
                       message = FALSE, 
                       warning = FALSE, 
                       tidy.opts = list(width.cutoff = 60),
                       tidy = TRUE)

```

# Packages for today

```{r}

# load packages
pacman::p_load(tidyverse,
               corrr,
               psych,
               tidyplots,
               HistData) # new data package

```

# Dataset

[Galton's data on the heights of parents and their children, by child](https://friendly.github.io/HistData/reference/GaltonFamilies.html)

This data set lists the individual observations for 934 children in 205 families on which Galton (1886) based his cross-tabulation.

I have wrangled two datasets for you to choose from today

  - "father_son_height.csv"
  - "mother_daughter_height.csv"

Both of these are on Canvas for download.   
**You are free to select either for the exercise today.**
  
  
# Session outline

- Your job is work in groups to follow the steps outlined on Monday's lecture to
  - visualise each vectors distribution
  - visualise their relationship with one another
  - gather the relevant statistics to simulate data based on the source data
  - simulate 1000 new observations
  - plot the real against the simulated data

# Group work suggestions

- Work with someone new today!
- Use the terminal to display Monday's slides
- Delegate tasks as part of a team: this'll help to speed things up.
- As always, annotate and comment your RMarkdown scripts
- Strive to keep the code tidy
- Good luck!

<!--

-   Today, we are continuing to learn about statistics through data
    simulation.
-   We also continue to use the "heights_and_weights.csv" dataset we
    used for the session on simulating correlation data.
-   We are moving on to look at how you can estimate values given the
    statistics of simple regression.
    

# Regression Theory

-   Regression can be thought of as a process of generating a line of
    best fit given the data.
-   Rather than draw by hand, a procedure called **least squares
    regression** can be used to reliably draw this line.

## Lest squares regression

The line is adjusted to minimise the SSE (sum of squared deviations; $e_{i}^{2}$) of each point from the line:

Minimise SSE =
$min \underset{i=1}{\stackrel{n}{\sum}}e_{i}^{2} = min \underset{i=1}{\stackrel{n}{\sum}}(y_{i} - \hat{y}_{i})^2$

-   $y_{i}$ = observed value
-   $\hat{y}_{i}$ = predicted value
-   $e_{i}$ = error
-   $n$ = number of observations
-   $\underset{i=1}{\stackrel{n}{\sum}}$ = summation of a sequence of terms indexed by $i$, starting from $i = 1$ and ending at $i = n$

# Regression formula

$$Y_i = \beta_0 + \beta_1 X_i + e_i$$

Where...

-   $y_{i}$ = $i$th observation of the dependent variable,
    $i = 1,2,...,n$
-   $x_{i}$ = $i$th observation for the independent variable,
    $i = 1,2,...n$
-   $\beta_{0}$ = intercept or constant
-   $\beta_{1}$ = the slope or gradient of the predictor/independent
    variable
-   $\epsilon_{i}$ = error or residual of the $i$th observation
-   $n$ = total number of observations

# Illustration of regression

![](images/RM3_regression.png){left="0" width="580" height="590"}

# Calculating coefficients

$\beta_{0}$ and $\beta_{1}$ are **coefficients** and can be estimated
using the following equations:

$\hat{\beta}_{1} = \frac{\underset{i=1}{\stackrel{n}{\sum}}(x_{i} - \overline{x})(y_{i} - \overline{y})}{\underset{i=1}{\stackrel{n}{\sum}}(x_{i} - \overline{x})^2}$

$\beta_{0} = \overline{y} - \hat{\beta}_{1}\overline{x}$    


# what about the relationship between correlation and regression?

-   Correlation tells us about the strength and relationship between two
    variables.
-   Regression (on the other hand) predicts the value of one variable
    based on the value of another variable.
-   For example, in the heights and weights data, we can use regression
    modelling to *predict someone's weight given their height*.

# Set up code for the session

```{r, eval = F}
# Load packages
library(tidyverse)
library(corrr)


# Read in the data
handw <-
  read_csv("data_raw/heights_and_weights.csv",
           col_types = "dd")


# Add log transformed vectors to dataset
handw_log <-
  handw |>
  mutate(hlog = log(height_in),  
         wlog = log(weight_lbs))


# Set seed for reproducibility
set.seed(123)


# Change the output format
options(scipen = 999)
```

# Remind me, why are we using the log of the data?

::: panel-tabset
### Raw data

```{r echo=FALSE, eval=F}
# generate the scatter plot with ggplot2
ggplot(data = handw,                                    # specify data source
       mapping = aes(x = height_in, y = weight_lbs)) +  # specify x and y axis
  geom_point(alpha = .6) +                              # modify transparency of points
  labs(x = "\nHeight (inches)",                         # supply axis label names
       y = "Weight (pounds)\n",
       title = "Raw height and weight data")
```

### Normalised data

```{r echo=FALSE, eval=F}
# generate a scatter plot of the log data
ggplot(data = handw_log,                      # don't forget to enter the new dataset              
       mapping = aes(x = hlog, y = wlog)) +  
  geom_point(alpha = .6) +                              
  labs(x = "\nLog(Height)",                           
       y = "log(Weight)\n",
       title = "log transformed height and weight data")

```
:::

# Rationale for taking the log

-   When we take the log of the data we normalise it, to stabilise the
    variance in our vectors.

-   This is particularly useful if the data are highly skewed or show
    **heteroscedasticity**

    -   When the difference between the observed and predicted values
        (i.e., the residuals $e_i$) is not constant across all levels of
        the independent variables.

-   Normalised data is more suitable for statistical analysis that
    assume normality, such as linear regression.


# Using regression to make predictions based on height and weight data

-   $Y_i$ = prediction of a person $i$'s weight

-   $X_i$ = observed height

-   $\beta_0$ = y-intercept

-   $\beta_1$ = slope parameter

-   $e_i$ = residuals

    -   Note, it is assumed that $e_i$ comes from a normal distribution
        with a mean of zero and variance $\sigma^2$.

# Making predictions using the available statistics

To estimate the parameters of the regression between the y-intercept
($\beta_0$) and the slope ($\beta_1$) all we need is the:

-   Mean estimates for $X$ and $Y$, or $\hat{\mu_x}$ & $\hat{\mu_y}$
-   Standard deviations for $X$ and $Y$, or $\hat{\sigma_x}$ &
    $\hat{\sigma_y}$
-   Correlations between $X$ and $Y$, or $\hat{\rho}$
-   Note: the $\hat{hat}$ denotes an estimate of a population parameter.

So, the statistics required to estimate $\beta_0$ and $\beta_1$ are much
the same as we used for simulating correlational data.

# A reminder of our previous calculations

-   $\hat{\mu_x} = 4.11, \hat{\sigma_x} = 0.26$ (estimated mean and SD
    of log height)

-   $\hat{\mu_y} = 4.74, \hat{\sigma_y} = 0.65$ (estimated mean and SD
    of log weight)

-   $\hat{\rho_{xy}} = 0.96$ (estimated correlation between the two)

# Estimating the slope $\beta_1$

-   Let's start by estimating the value of the slope $\beta_1$.
-   Importantly, $\beta_1$ can be expressed in terms of the correlation
    coefficient $\rho$ times the ratio of the standard deviations of $Y$
    and $X$.

$$\beta_1 = \rho\frac{ \sigma_Y}{\sigma_X}$$

# Plugging in the numbers

-   Now, you can use the estimates of log height and log weight, to
    estimate the slope:

$$\beta_1 = \rho\frac{ \sigma_Y}{\sigma_X}$$

In R Code:

```{r estimate slope, eval = F}

# Estimate slope using formula
b1 <- .96 * (.65 / .26)
b1 # 2.4

```

# Using the Axis to fill in the blanks: part 1

-   For mathematical reasons, the regression line is guaranteed to go
    through the point corresponding to the mean of both $X$ and $Y$,
    i.e., the point ($\mu_x$, $\mu_y$).

-   One way to think about this is that the regression line pivots
    around that point depending on the slope ($\beta_1$).

-   We also know that $\beta_0$ is the y-intercept, where the line
    crosses the vertical axis at $X$ = 0.

# Using the Axis to fill in the blanks: part 2

-   From all of this information we can calculate $\beta_0$.

-   Remember that $\beta_1$ tells you that for each change in $X$ you
    have a corresponding change of **2.4** for $Y$, and that the line
    goes through points ($\mu_x$, $\mu_y$) as well as the y-intercept
    (0, $\beta_0$).

# Re-framing the calculations

-   Think about stepping back unit-by-unit from the mean of $X = \mu_x$
    to $X = 0$.

-   At $X =\mu_x$, $Y = 4.74$, because as stated earlier, the regression
    line is guaranteed to go through the point corresponding to the mean
    of both $X$ and $Y$, i.e., the point ($\mu_x$, $\mu_y$) or (4.11,
    4.74).

-   Each unit step you take backward in the $X$ dimension, $Y$ will
    reduce by $\beta_1 = 2.4$ units.

-   When you get to zero, $Y$ will have dropped from $\mu_y$ to
    $\mu_y - \mu_x\beta_1$.

# The Solution

-   With all of the above considerations taking into account the
    solution is $\beta_0 = \mu_y - \mu_x\beta_1$.

-   Using this information we can calculate the slope value:
    $\beta_0 = 4.74 - 4.11 \times 2.4 = -5.124$

-   Now we have the following statistics:

    -   $\beta_1 = 2.4$

    -   $\mu_x = 4.11$

    -   $\mu_y - 4.74$

    -   $\beta_0 = -5.124$

-   So the formula becomes: $$Y_i = -5.124 + 2.4X_i + e_i$$

# Checking the results

-   To check the results, let's first run a regression on the log
    transformed data using `lm()`, which estimates parameters using
    *ordinary least squares (OLS) regression*.
-   In OLS regression, the goal is to find the line that best fits the
    data by minimizing the sum of the squared differences between the
    observed values of the dependent variable and the values predicted
    by the regression line.
-   Note, you are interested in the `Estimate` values.

```{r, eval=F}
#| echo: true
#| output-location: slide
summary(lm(wlog ~ hlog,
           data = handw_log))
```

# Matching the regression output to our calculations

From the model output:

-   Estimated slope parameter $\beta_1 = 2.433$ (2.4 from our
    calculations)

-   Estimated y-intercept $\beta_0 = -5.269$ (-5.124 from our
    calculations)

-   These don't match exactly because of the rounding we've used in our
    calculations.

# Checking your regression estimate with a plot

Another way to check the accuracy of your regression calculations is to
superimpose the regression line on the scatter-plot of the log
transformed data.

```{r, eval=F}
#| echo: true
#| output-location: slide
ggplot(data = handw_log, 
       aes(hlog, wlog)) +
  geom_point(alpha = .3)+
  geom_abline(intercept = -5.124, 
              slope = 2.4, 
              colour = 'purple') +
  labs(title = "Superimposed regression line onto log transformed data",
       x = "\nlog(height)", 
       y = "log(weight)\n")
```

# Predicting someone's weight given their height

-   Say we want to predict the weight of someone who is 69inches or
    175cm (average height of a person from the US). Let's plug the log
    of this value (4.23) into our regression formula:

$Y_i = -5.124 + 2.4 \times 4.23 + e_i$

-   Note: We do not need to provide the residuals ($e_i$) as they are
    estimated from the regression equation.

$Y_i = 5.028$

$exp(5.028) = 152.63lbs = 69.2kg$

-   So, our regression model predicts that someone who is 175cm tall
    would weigh 69.2kg.

# A little more about $e_i$

-   Conventionally, $e_i$ come from a normal distribution with $\mu = 0$
    and variance $\sigma^2$.
-   $e_i$ are important for assessing the model's performance and
    diagnostic purposes but they are not necessary for making
    predictions using the regression equation.

# $R^{2}$ Coefficient of determination

To this point we have created a regression equation and used it to
predict someone's weight given their height. But, we also want to know
how good a fit our equation is given the data. To calculate this we use
the ***coefficient of determination*** ($R^2$):

$R^2 = \frac{\text{Sum of Squares Explained by Regression (SSR)}}{\text{Total Sum of Squares (before regression)(TSS)}} = \frac{\underset{i=1}{\stackrel{n}{\sum}}(\hat{y}_{i} - \overline{y})^2}{\underset{i=1}{\stackrel{n}{\sum}}(y_{i} - \overline{y})^{2}}$

The total variation (TSS) in the dependent variable (weight) is split
into two parts: the part explained (SSR) by the association between the
dependent (weight) and independent (height) variables, and the part that
is unexplained (SSE) since the relationship is never perfect and there
are always some residuals.


# $R^{2}$ Thresholds

The higher the $R^{2}$ value (at least $70\%$) the better the model fit.
A reasonable model fit would be more $R^{2} >= 60\%$.

The coefficient of determination can take values between 0 and 1, but is
commonly reported as a percentage, as it represent the proportion of the
variation in the dependent variable ($Y_{i}$) which is explained by the
predictor/independent variable ($X_{i}$).


# Exploring the Errors

To diagnose the quality of the model further we need to look at the
errors or residuals $(y_{i} - \hat{y}_{i})$ after running the regression
model.

Model residuals should be **randomly scattered** with **no extreme
values** and should have a **mean of zero**.

Should these requirements not be met we would have to further
investigate whether there is information in the residuals that could be
covered by the model or be considered a cause for concern.

A histogram of the residuals and normal probability plot can help you
decide how well your residuals fit into the model.

# Histogram of model residuals

::: panel-tabset
### Raw data: residuals plot code

```{r, eval=F}

# Create model for raw data
mod1 <-
  lm(weight_lbs ~ height_in,
     data = handw)

# Create residuals object
residuals_df <- 
  mod1$residuals |>
  as_tibble() |>
  rename(residuals = value)

# Plot the data
ggplot(residuals_df, aes(x = residuals)) +
  geom_histogram(binwidth = 10) +
  labs(title = "Histogram of Residuals",
       x = "Residuals",
       y = "Frequency")


```

### Raw data: residuals plot output

```{r, echo=FALSE, eval=F}

# Create model for raw data
mod1 <-
  lm(weight_lbs ~ height_in,
     data = handw)

# Create residuals object
residuals_df <- 
  mod1$residuals |>
  as_tibble() |>
  rename(residuals = value)

# Plot the data
ggplot(residuals_df, aes(x = residuals)) +
  geom_histogram(binwidth = 10) +
  labs(title = "Histogram of Residuals",
       x = "Residuals",
       y = "Frequency")


```

### Transformed data: residuals plot

```{r, echo=FALSE, eval=F}

# Create model for transformed data
mod2 <-
  lm(wlog ~ hlog,
     data = handw_log)

# Create residuals object
residuals_df <- 
  mod2$residuals |>
  as_tibble() |>
  rename(residuals = value)

# Plot the data
ggplot(residuals_df, aes(x = residuals)) +
  geom_histogram(binwidth = 10) +
  labs(title = "Histogram of Log Transformed Residuals",
       x = "Log(residuals)",
       y = "Frequency")


```
:::

# Normal Probability-Probability (P-P) Plots

::: panel-tabset
### Raw data: Normal PP plot code

```{r, eval=F}

# Extract residuals
residuals <-
  mod1$residuals

# Calculate theoretical quantiles
mod1_quantiles <-
  qqnorm(residuals, plot.it = FALSE)$x

# Create a data frame with residuals and theoretical quantiles
pp_df <- 
  data.frame(
  residuals = residuals,
  theoretical_quantiles = mod1_quantiles
)

# Plot the P-P plot
ggplot(pp_df, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "Normal P-P Plot of Residuals",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal()


```

### Raw data: Normal PP plot output

```{r echo=FALSE, eval=F}

# Extract residuals
residuals <-
  mod1$residuals

# Calculate theoretical quantiles
mod1_quantiles <-
  qqnorm(residuals, plot.it = FALSE)$x

# Create a data frame with residuals and theoretical quantiles
pp_df <- 
  data.frame(
  residuals = residuals,
  theoretical_quantiles = mod1_quantiles
)

# Plot the P-P plot
ggplot(pp_df, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "Normal P-P Plot of Residuals",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal()


```

### Transformed data: Normal PP plot ouput

```{r, echo=FALSE, eval=F}

# Extract residuals
residuals <-
  mod2$residuals

# Calculate theoretical quantiles
mod2_quantiles <-
  qqnorm(residuals, plot.it = FALSE)$x

# Create a data frame with residuals and theoretical quantiles
pp_df <- 
  data.frame(
  residuals = residuals,
  theoretical_quantiles = mod2_quantiles
)

# Plot the P-P plot
ggplot(pp_df, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "Normal P-P Plot of log(Residuals)",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal()


```
:::

# Testing the Significance of the model and its coefficients

We can use statistical tests to determine how well we are approximating the population parameters with those in our model. 
To do this we can use two methods:

- Analysis of variance (ANOVA): tests overall significance of the model
- $t$-test: test the individual significance of the coefficients.


## Testing the overall significance of the model

Testing the overall significance of the model evaluates how well the independent variables reliable predict the dependent variable. 
We can create hypotheses to make our statistical inferences:

$H_{0}$: The regression model does not explain a significant proportion of the variance in weight (our DV). 

VS

$H_{1}$: The regression model does explain a significant proportion of the variation in weight. 

And we test the above using the $F$-distribution. 


# ANOVA for regression modelling

::: panel-tabset
### ANOVA for regression

| Source of Variation | Sum of Squares (SS) | Degrees of Freedom (df) | Mean Square (MS) | F-Statistic (F)               |
|---------------------|----------------------|-------------------------|------------------|-------------------------------|
| Model               | $\text{SS}_{\text{model}} = \underset{i=1}{\stackrel{n}{\sum}}(\hat{y}_i - \bar{y})^2$ | \( k \)                   | $\text{MSR} = \frac{\text{SSR}}{k}$ | $\frac{\text{MSR}}{\text{MSE}}$ |
| Residual            | $\text{SS}_{\text{residual}} = \underset{i=1}{\stackrel{n}{\sum}}(y_i - \hat{y}_i)^2$ | \( n - k - 1 \)           | $\text{MSE} = \frac{\text{SSE}}{n - k - 1}$ |                               |
| Total               | $\text{TSS} = SSR + SSE = \underset{i=1}{\stackrel{n}{\sum}}(y_i - \bar{y})^2$ | \( n - 1 \)               |                              |                               |

### ANOVA for regression: key

Where...

- $n$ = total number of observations
- $k$ = total number of independent variables
- $y$ = the observed values
- $\overline{y}$ = mean of the dependent variables
- $\hat{y}_{i}$ = the estimated values

:::

# ANOVA for log transformed model

```{r, eval=F}

# Run an ANOVA on our log transformed model
anova_results <-
  anova(mod2)

print(anova_results)

# Calculate TSS
ss_model <-
  anova_results["hlog", "Sum Sq"]

ss_residual <-
  anova_results["Residuals", "Sum Sq"]

tss <- 
  ss_model + ss_residual

print(tss)

```

## Interpreting the results of ANOVA

We can see from the output that `hlog` that the slope of the line is significant difference from 0, where $p < 0.001$. 
Thus we reject $H_{0}$. 


# Testing individual coefficients

It is important to determine whether each coefficient is relevant for our model.
We can do this by checking if it is significantly different from 0.

This can be done by dividing the coefficient by its standard error and comparing the absolute value of that $t$-ratio to the relevant statistic from the $t$-distribution. 

The hypothesis being tested here is as follows:

$H_{0}: \beta_{i}=0$

VS

$H_{a}: \beta_{i}\neq0$


#### UP TO PAGE 235 RM3 TEXTBOOK


# Related Reading

[Learning Statistical Models Through Simulation in R: Correlation and
Regression](https://psyteachr.github.io/stat-models-v1/correlation-and-regression.html)

-->
