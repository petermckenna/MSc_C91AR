---
title: "Lecture 8: The Relationship Between Correlation and Regression"
subtitle: "C91AR: Advanced Statistics using R"
title-slide-attributes:
    data-background-image: images/anime_programming.jpg
    data-background-size: 20%
    data-background-position: "50% 8%"
author: "Dr Pete McKenna"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
format: 
  revealjs:
    code-block-height: 300px
    slide-number: true
    logo: sossdoc-logo.png
    css: peter_style.css
    theme: [default, my_quarto_theme.scss]
editor: 
  markdown: 
    wrap: 72
bibliography: ref.bib 
---

```{r setup, include=FALSE}

# markdown formatting
knitr::opts_chunk$set(echo = TRUE)

```

# Reading for today

@Barr_2025

@Canduela_Raeside_2020


# Session outline

-   Today, we are continuing to learn about statistics through data
    simulation.
-   We also continue to use the "heights_and_weights.csv" dataset we
    used for the session on simulating correlation data.
-   We are moving on to look at how you can estimate values given the
    statistics of simple regression.
    

# Regression Theory

-   Regression can be thought of as a process of generating a line of
    best fit given the data.
-   Rather than draw by hand, a procedure called **least squares
    regression** can be used to reliably draw this line.

## Least squares regression

The line is adjusted to minimise the SSE (sum of squared deviations; $e_{i}^{2}$) of each point from the line:

Minimise SSE =
$min \underset{i=1}{\stackrel{n}{\sum}}e_{i}^{2} = min \underset{i=1}{\stackrel{n}{\sum}}(y_{i} - \hat{y}_{i})^2$

-   $y_{i}$ = observed value
-   $\hat{y}_{i}$ = predicted value
-   $e_{i}$ = error
-   $n$ = number of observations
-   $\underset{i=1}{\stackrel{n}{\sum}}$ = summation of a sequence of terms indexed by $i$, starting from $i = 1$ and ending at $i = n$


# Illustration of regression

![](images/RM3_regression.png){left="0" width="580" height="590"}

- So, least squares regression is summation of all of the squared differences ($e_{i}$) between the observed ($y_{i}$) and predicted ($\hat{y}_{i}$) values of $y$


# Regression formula

$$Y_i = \beta_0 + \beta_1 X_i + e_i$$

Where...

-   $y_{i}$ = $i$th observation of the dependent variable,
    $i = 1,2,...,n$
-   $x_{i}$ = $i$th observation for the independent variable,
    $i = 1,2,...n$
-   $\beta_{0}$ = intercept or constant
-   $\beta_{1}$ = the slope or gradient of the predictor/independent
    variable
-   $\epsilon_{i}$ = error or residual of the $i$th observation
-   $n$ = total number of observations

<!--
# Calculating coefficients

$\beta_{0}$ and $\beta_{1}$ are **coefficients** and can be estimated
using the following equations:

$\hat{\beta}_{1} = \frac{\underset{i=1}{\stackrel{n}{\sum}}(x_{i} - \overline{x})(y_{i} - \overline{y})}{\underset{i=1}{\stackrel{n}{\sum}}(x_{i} - \overline{x})^2}$

$\beta_{0} = \overline{y} - \hat{\beta}_{1}\overline{x}$    
-->

# The relationship between correlation and regression?

-   Correlation tells us about the strength and relationship between two
    variables.
-   Regression (on the other hand) predicts the value of one variable
    based on the value of another variable.
-   For example, in the heights and weights data, we can use regression
    modelling to *predict someone's weight given their height*.

# Reproducibility & notation

```{r}

# Set seed for reproducibility
set.seed(123)

# Change the output format
options(scipen = 999)

```


# Load packages

```{r}
# Load packages
pacman::p_load(tidyverse,
               corrr,
               tidyplots)

```

# Data setup

```{r}

# Read in the data
handw <-
  read_csv("data_raw/heights_and_weights.csv",
           col_types = "dd")


# Add log transformed vectors to dataset
handw_log <-
  handw |>
  mutate(hlog = log(height_in),  
         wlog = log(weight_lbs))

```

# Remind me, why are we using the log of the data?

# Raw data

```{r echo=FALSE}
# generate the scatter plot with ggplot2
ggplot(data = handw,                                    # specify data source
       mapping = aes(x = height_in, y = weight_lbs)) +  # specify x and y axis
  geom_point(alpha = .6) +                              # modify transparency of points
  labs(x = "\nHeight (inches)",                         # supply axis label names
       y = "Weight (pounds)\n",
       title = "Raw height and weight data")
```

# Normalised data

```{r echo=FALSE}
# generate a scatter plot of the log data
ggplot(data = handw_log,                      # don't forget to enter the new dataset              
       mapping = aes(x = hlog, y = wlog)) +  
  geom_point(alpha = .6) +                              
  labs(x = "\nLog(Height)",                           
       y = "log(Weight)\n",
       title = "log transformed height and weight data")

```


# Rationale for taking the log

-   When we take the log of the data we normalise it, to stabilise the
    variance in our vectors.
-   This is particularly useful if the data are highly skewed or show
    **heteroscedasticity**
    -   When the difference between the observed and predicted values
        (i.e., the residuals $e_i$) is not constant across all levels of
        the independent variables.
    -   For the heights and weights data, you can see that the residuals increase as the values for height and weight approach the upper limit 
-   Normalised data is more suitable for statistical analysis that
    assume normality, such as linear regression.


# Using regression to make predictions based on height and weight data

**Regression formula**

$$Y_i = \beta_0 + \beta_1 X_i + e_i$$

**In the context of predicting height from weight**

-   $Y_i$ = prediction of a person $i$'s weight

-   $X_i$ = observed height

-   $\beta_0$ = y-intercept

-   $\beta_1$ = slope parameter

-   $e_i$ = residuals

    -   Note, it is assumed that $e_i$ comes from a normal distribution
        with a mean of zero and variance $\sigma^2$.
        


# Making predictions using the available statistics

To estimate the parameters of the regression between the y-intercept
($\beta_0$) and the slope ($\beta_1$) all we need is the:

-   Mean estimates for $X$ and $Y$, denoted as $\hat{\mu_x}$ & $\hat{\mu_y}$
-   Standard deviations for $X$ and $Y$, denoted as $\hat{\sigma_x}$ &
    $\hat{\sigma_y}$
-   Correlations between $X$ and $Y$, denoted as $\hat{\rho}$
-   Note: the $\hat{hat}$ denotes an estimate of a population parameter.

So, the statistics required to estimate $\beta_0$ and $\beta_1$ are much
the same as we used for simulating correlational data.

# A reminder of our previous calculations

-   $\hat{\mu_x} = 4.11, \hat{\sigma_x} = 0.26$ (estimated mean and SD
    of log height)

-   $\hat{\mu_y} = 4.74, \hat{\sigma_y} = 0.65$ (estimated mean and SD
    of log weight)

-   $\hat{\rho_{xy}} = 0.96$ (estimated correlation between the two)

# Estimating the slope $\beta_1$

-   Let's start by estimating the value of the slope $\beta_1$.
-   Importantly, $\beta_1$ can be expressed in terms of the correlation
    coefficient $\rho$ times the ratio of the standard deviations of $Y$
    and $X$.

$$\beta_1 = \rho\frac{ \sigma_Y}{\sigma_X}$$

# Plugging in the numbers

-   Now, you can use the estimates of log height and log weight, to
    estimate the slope:

$$\beta_1 = \rho\frac{ \sigma_Y}{\sigma_X}$$

In R Code:

```{r estimate slope}

# Estimate slope using formula
b1 <- .96 * (.65 / .26)
b1 # 2.4

```

# Using the Axis to fill in the blanks: part 1

-   For mathematical reasons, the regression line is **guaranteed to go
    through the point corresponding to the mean of both $X$ and $Y$,
    i.e., the point ($\mu_x$, $\mu_y$)**.

-   One way to think about this is that the regression line pivots
    around that point depending on the slope ($\beta_1$).

-   We also know that $\beta_0$ is the y-intercept, where the line
    crosses the vertical axis at $X$ = 0.
    
    
# Regression line passing through variable means ($\mu_x$, $\mu_y$)
    
```{r, echo=FALSE}

# data = handw_log                      
# vars: x = hlog, y = wlog 

# Fit a linear model
model <- 
  lm(wlog ~ hlog, data = handw_log)

# Get the means of log-transformed height and weight
mean_hlog <- 
  mean(handw_log$hlog)
mean_wlog <- 
  mean(handw_log$wlog)

# Create the plot
ggplot(handw_log, aes(x = hlog, 
                 y = wlog)) +
  geom_point(color = "green", 
             alpha = 0.5) +  # Scatter plot of the data points
  geom_smooth(method = "lm", 
              color = "red", 
              se = FALSE) +  # Regression line
  geom_vline(xintercept = mean_hlog, 
             linetype = "dashed", 
             color = "blue") +  # Mean of hlog
  geom_hline(yintercept = mean_wlog, 
             linetype = "dashed", 
             color = "blue") +  # Mean of wlog
  geom_point(aes(x = mean_hlog, 
                 y = mean_wlog), 
             color = "purple", size = 3) +  # Mean point
  labs(
    title = "Simple Linear Regression with Log-Transformed Data",
    subtitle = "Regression line passes through the mean of log-transformed height (4.11) and weight (4.74)",
    x = "\nLog of Height (in inches)",
    y = "Log of Weight (in lbs)\n"
  ) +
  annotate("text", 
           x = mean_hlog, 
           y = mean_wlog, 
           label = "Mean (hlog, wlog)", 
           vjust = -1, color = "purple") +
  theme_minimal()

```

<!--

This code will generate a plot with:
- A scatter plot of the log-transformed data points in blue.
- A red regression line.
- Green dashed lines representing the means of log-transformed height and weight.
- A purple point at the intersection of the means of log-transformed height and weight, labeled "Mean (hlog, wlog)".

Feel free to run this code in your R environment and let me know if you need any further assistance!


This code will generate a plot with:
- A scatter plot of the data points in blue.
- A red regression line.
- Green dashed lines representing the means of X and Y.
- A purple point at the intersection of the means of X and Y, labeled "Mean (X, Y)".

Feel free to run this code in your R environment and let me know if you need any further assistance!

-->


# Using the Axis to fill in the blanks: part 2

-   From all of this information we can calculate $\beta_0$.

-   Remember that $\beta_1$ tells you that for each change in $X$ you
    have a corresponding change of **2.4** for $Y$, and that the line
    goes through points ($\mu_x$, $\mu_y$) as well as the y-intercept
    (0, $\beta_0$).

# Re-framing the calculations

-   Think about stepping back unit-by-unit from the mean of $X = \mu_x$
    to $X = 0$.

-   At $X =\mu_x$, $Y = 4.74$, because as stated earlier, the regression
    line is guaranteed to go through the point corresponding to the mean
    of both $X$ and $Y$, i.e., the point ($\mu_x$, $\mu_y$) or (4.11,
    4.74).

-   Each unit step you take backward in the $X$ dimension, $Y$ will
    reduce by $\beta_1 = 2.4$ units.

-   When you get to zero, $Y$ will have dropped from $\mu_y$ to
    $\mu_y - \mu_x\beta_1$.
    
    
# Illustrating this relationship    

![](images/regression_equation.png)


```{r, include=FALSE}
# data = handw_log                      
# vars: x = hlog, y = wlog 

# Fit a linear model
model <- 
  lm(wlog ~ hlog, data = handw_log)

# Get the means of log-transformed height and weight
mean_hlog <- 
  mean(handw_log$hlog)
mean_wlog <- 
  mean(handw_log$wlog)

# Create the plot
ggplot(handw_log, aes(x = hlog, 
                 y = wlog)) +
  geom_point(color = "green", 
             alpha = 0.5) +  # Scatter plot of the data points
  geom_smooth(method = "lm", 
              color = "red", 
              se = FALSE) +  # Regression line
  geom_vline(xintercept = mean_hlog, 
             linetype = "dashed", 
             color = "blue") +  # Mean of hlog
  geom_hline(yintercept = mean_wlog, 
             linetype = "dashed", 
             color = "blue") +  # Mean of wlog
  geom_point(aes(x = mean_hlog, 
                 y = mean_wlog), 
             color = "purple", size = 3) +  # Mean point
  labs(
    title = "Simple Linear Regression with Log-Transformed Data",
    subtitle = "Regression line passes through the mean of log-transformed height (4.11) and weight (4.74)",
    x = "\nLog of Height (in inches)",
    y = "Log of Weight (in lbs)\n"
  ) +
  annotate("text", 
           x = mean_hlog, 
           y = mean_wlog, 
           label = "Mean (hlog, wlog)", 
           vjust = -1, color = "purple") +
  annotate("text", 
           x = mean_hlog - 0.5, 
           y = mean_wlog - 2.4 * 0.5, 
           label = "Each unit step backward in X reduces Y by 2.4 units", 
           vjust = -1, color = "black") +  # Annotation for slope effect
  annotate("text", 
           x = 0, 
           y = mean_wlog - mean_hlog * 2.4, 
           label = "Arriving at zero reduces mean of Y to mean of Y - mean of X * 2.4", 
           vjust = -1, color = "black") +  # Annotation for mean reduction
  xlim(-5, 15) +  # Extend x-axis limits
  ylim(-20, 50) +  # Extend y-axis limits
  theme_minimal()

```    
    

# The Solution

-   With all of the above considerations taking into account the
    solution is $\beta_0 = \mu_y - \mu_x\beta_1$.

-   Using this information we can calculate the slope value:
    $\beta_0 = 4.74 - 4.11 \times 2.4 = -5.124$

-   Now we have the following statistics:

    -   $\beta_1 = 2.4$

    -   $\mu_x = 4.11$

    -   $\mu_y - 4.74$

    -   $\beta_0 = -5.124$
    

# Plugging in the numbers

So..

$$Y_i = \beta_0 + \beta_1 X_i + e_i$$
Becomes...  

$$Y_i = -5.124 + 2.4X_i + e_i$$

# Checking the results

-   To check the results, let's first run a regression on the log
    transformed data using `lm()`, which estimates parameters using
    *ordinary least squares (OLS) regression*.
-   In OLS regression, the goal is to find the line that best fits the
    data by minimizing the sum of the squared differences between the
    observed values of the dependent variable and the values predicted
    by the regression line.
-   Note, you are interested in the `Estimate` values.

```{r}
#| echo: true
#| output-location: slide
summary(lm(wlog ~ hlog,
           data = handw_log))
```

# Matching the regression output to our calculations

From the model output:

-   Estimated slope parameter $\beta_1 = 2.433$ (2.4 from our
    calculations)

-   Estimated y-intercept $\beta_0 = -5.269$ (-5.124 from our
    calculations)

-   These don't match exactly because of the rounding we've used in our
    calculations.

# Checking your regression estimate with a plot

Another way to check the accuracy of your regression calculations is to
superimpose the regression line on the scatter-plot of the log
transformed data.

```{r}
#| echo: true
#| output-location: slide
ggplot(data = handw_log, 
       aes(hlog, wlog)) +
  geom_point(alpha = .3)+
  geom_abline(intercept = -5.124, 
              slope = 2.4, 
              colour = 'purple') +
  labs(title = "Superimposed regression line onto log transformed data",
       x = "\nlog(height)", 
       y = "log(weight)\n")
```

# Predicting someone's weight given their height

-   Say we want to predict the weight of someone who is 69inches or
    175cm (average height of a person from the US). Let's plug the log
    of this value (4.23) into our regression formula:

$Y_i = -5.124 + 2.4 \times 4.23 + e_i$

-   Note: We do not need to provide the residuals ($e_i$) as they are
    estimated from the regression equation.

$Y_i = 5.028$

$exp(5.028) = 152.63lbs = 69.2kg$

-   So, our regression model predicts that someone who is 175cm tall
    would weigh 69.2kg.

# A little more about $e_i$

-   Conventionally, $e_i$ come from a normal distribution with $\mu = 0$
    and variance $\sigma^2$.
-   $e_i$ are important for assessing the model's performance and
    diagnostic purposes but they are not necessary for making
    predictions using the regression equation.
    
# Model fit

- We did not cover how to test the model's fit today
- This is covered in the RM3 textbook, and there are good resources elsewhere
- I prioritised familiarity with the modelling process over assumption checking
    
# Roundup

- Today I have shown you how to calculate a simple regression model by hand using formula
- The aim was to unveil some of the computation that goes on behind the scene to help you understand what regression analysis is actually doing. 
- Using this method, we also made a prediction about someone's height given their weight. 
- On Monday, we are going to look at multiple regression

<!--

# $R^{2}$ Coefficient of determination

To this point we have created a regression equation and used it to
predict someone's weight given their height. But, we also want to know
how good a fit our equation is given the data. To calculate this we use
the ***coefficient of determination*** ($R^2$):

$R^2 = \frac{\text{Sum of Squares Explained by Regression (SSR)}}{\text{Total Sum of Squares (before regression)(TSS)}} = \frac{\underset{i=1}{\stackrel{n}{\sum}}(\hat{y}_{i} - \overline{y})^2}{\underset{i=1}{\stackrel{n}{\sum}}(y_{i} - \overline{y})^{2}}$

The total variation (TSS) in the dependent variable (weight) is split
into two parts: the part explained (SSR) by the association between the
dependent (weight) and independent (height) variables, and the part that
is unexplained (SSE) since the relationship is never perfect and there
are always some residuals.


# $R^{2}$ Thresholds

The higher the $R^{2}$ value (at least $70\%$) the better the model fit.
A reasonable model fit would be more $R^{2} >= 60\%$.

The coefficient of determination can take values between 0 and 1, but is
commonly reported as a percentage, as it represent the proportion of the
variation in the dependent variable ($Y_{i}$) which is explained by the
predictor/independent variable ($X_{i}$).


# Exploring the Errors

To diagnose the quality of the model further we need to look at the
errors or residuals $(y_{i} - \hat{y}_{i})$ after running the regression
model.

Model residuals should be **randomly scattered** with **no extreme
values** and should have a **mean of zero**.

Should these requirements not be met we would have to further
investigate whether there is information in the residuals that could be
covered by the model or be considered a cause for concern.

A histogram of the residuals and normal probability plot can help you
decide how well your residuals fit into the model.

# Histogram of model residuals

Raw data: residuals plot code

```{r, eval = F}

# Create model for raw data
mod1 <-
  lm(weight_lbs ~ height_in,
     data = handw)

# Create residuals object
residuals_df <- 
  mod1$residuals |>
  as_tibble() |>
  rename(residuals = value)

# Plot the data
ggplot(residuals_df, aes(x = residuals)) +
  geom_histogram(binwidth = 10) +
  labs(title = "Histogram of Residuals",
       x = "Residuals",
       y = "Frequency")


```

# Raw data: residuals plot output

```{r, echo=FALSE, eval = F}

# Create model for raw data
mod1 <-
  lm(weight_lbs ~ height_in,
     data = handw)

# Create residuals object
residuals_df <- 
  mod1$residuals |>
  as_tibble() |>
  rename(residuals = value)

# Plot the data
ggplot(residuals_df, aes(x = residuals)) +
  geom_histogram(binwidth = 10) +
  labs(title = "Histogram of Residuals",
       x = "Residuals",
       y = "Frequency")


```

# Transformed data: residuals plot

```{r, echo=FALSE, eval = F}

# Create model for transformed data
mod2 <-
  lm(wlog ~ hlog,
     data = handw_log)

# Create residuals object
residuals_df <- 
  mod2$residuals |>
  as_tibble() |>
  rename(residuals = value)

# Plot the data
ggplot(residuals_df, aes(x = residuals)) +
  geom_histogram(bins = 30) +
  labs(title = "Histogram of Log Transformed Residuals",
       x = "Log(residuals)",
       y = "Frequency")


```


# Normal Probability-Probability (P-P) Plots

**Raw data: Normal PP plot code**

```{r, eval = F}

# Extract residuals
residuals <-
  mod1$residuals

# Calculate theoretical quantiles
mod1_quantiles <-
  qqnorm(residuals, plot.it = FALSE)$x

# Create a data frame with residuals and theoretical quantiles
pp_df <- 
  data.frame(
  residuals = residuals,
  theoretical_quantiles = mod1_quantiles
)

# Plot the P-P plot
ggplot(pp_df, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "Normal P-P Plot of Residuals",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal()


```

# Raw data: Normal PP plot output

```{r, echo=FALSE, eval = F}

# Extract residuals
residuals <-
  mod1$residuals

# Calculate theoretical quantiles
mod1_quantiles <-
  qqnorm(residuals, plot.it = FALSE)$x

# Create a data frame with residuals and theoretical quantiles
pp_df <- 
  data.frame(
  residuals = residuals,
  theoretical_quantiles = mod1_quantiles
)

# Plot the P-P plot
ggplot(pp_df, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "Normal P-P Plot of Residuals",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal()


```

# Transformed data: Normal PP plot ouput

```{r, echo=FALSE, eval = F}

# Extract residuals
residuals <-
  mod2$residuals

# Calculate theoretical quantiles
mod2_quantiles <-
  qqnorm(residuals, plot.it = FALSE)$x

# Create a data frame with residuals and theoretical quantiles
pp_df <- 
  data.frame(
  residuals = residuals,
  theoretical_quantiles = mod2_quantiles
)

# Plot the P-P plot
ggplot(pp_df, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "Normal P-P Plot of log(Residuals)",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal()


```


# Testing the Significance of the model and its coefficients

We can use statistical tests to determine how well we are approximating the population parameters with those in our model. 
To do this we can use two methods:

- Analysis of variance (ANOVA): tests overall significance of the model
- $t$-test: test the individual significance of the coefficients.


## Testing the overall significance of the model

Testing the overall significance of the model evaluates how well the independent variables reliable predict the dependent variable. 
We can create hypotheses to make our statistical inferences:

$H_{0}$: The regression model does not explain a significant proportion of the variance in weight (our DV). 

VS

$H_{1}$: The regression model does explain a significant proportion of the variation in weight. 

And we test the above using the $F$-distribution. 


# ANOVA for regression modelling

**ANOVA for regression**

| Source of Variation | Sum of Squares (SS) | Degrees of Freedom (df) | Mean Square (MS) | F-Statistic (F)               |
|---------------------|----------------------|-------------------------|------------------|-------------------------------|
| Model               | $\text{SS}_{\text{model}} = \underset{i=1}{\stackrel{n}{\sum}}(\hat{y}_i - \bar{y})^2$ | \( k \)                   | $\text{MSR} = \frac{\text{SSR}}{k}$ | $\frac{\text{MSR}}{\text{MSE}}$ |
| Residual            | $\text{SS}_{\text{residual}} = \underset{i=1}{\stackrel{n}{\sum}}(y_i - \hat{y}_i)^2$ | \( n - k - 1 \)           | $\text{MSE} = \frac{\text{SSE}}{n - k - 1}$ |                               |
| Total               | $\text{TSS} = SSR + SSE = \underset{i=1}{\stackrel{n}{\sum}}(y_i - \bar{y})^2$ | \( n - 1 \)               |                              |                               |
----

ANOVA for regression: key

- $n$ = total number of observations
- $k$ = total number of independent variables
- $y$ = the observed values
- $\overline{y}$ = mean of the dependent variables
- $\hat{y}_{i}$ = the estimated values


# ANOVA for log transformed model

```{r, eval = F}

# Run an ANOVA on our log transformed model
anova_results <-
  anova(mod2)

print(anova_results)

```

----

```{r, eval = F}

# Calculate TSS
ss_model <-
  anova_results["hlog", "Sum Sq"]

ss_residual <-
  anova_results["Residuals", "Sum Sq"]

tss <- 
  ss_model + ss_residual

print(tss)

```

## Interpreting the results of ANOVA

We can see from the output that `hlog` that the slope of the line is significant difference from 0, where $p < 0.001$. 
Thus we reject the null hypothesis ($H_{0}$). 


# Testing individual coefficients

It is important to determine whether each coefficient is relevant for our model.
We can do this by checking if it is significantly different from 0.

This can be done by dividing the coefficient by its standard error and comparing the absolute value of that $t$-ratio to the relevant statistic from the $t$-distribution. 

The hypothesis being tested here is as follows:

$H_{0}: \beta_{i}=0$

VS

$H_{a}: \beta_{i}\neq0$
-->

# References



